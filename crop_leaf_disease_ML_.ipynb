{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hudaor/leaf-disease-1/blob/main/crop_leaf_disease_ML_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qSjVz8FXSlpn"
      },
      "outputs": [],
      "source": [
        "#!find  /usr/local/lib/python3.7/dist-packages/ -name '~*' -exec rm -r {} \\;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xq3QauZb07Gj",
        "outputId": "c8493d8d-8d8e-43a5-bfaa-453b30eb8f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: frimcla in /usr/local/lib/python3.7/dist-packages (1.0.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from frimcla) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (from frimcla) (0.2.0)\n",
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.7/dist-packages (from frimcla) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from frimcla) (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from frimcla) (3.2.2)\n",
            "Requirement already satisfied: arff in /usr/local/lib/python3.7/dist-packages (from frimcla) (0.9)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from frimcla) (0.18.3)\n",
            "Requirement already satisfied: Keras==2.1.6 in /usr/local/lib/python3.7/dist-packages (from frimcla) (2.1.6)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from frimcla) (3.2)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.7/dist-packages (from frimcla) (0.5.4)\n",
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (from frimcla) (1.4.12)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from frimcla) (1.0.5)\n",
            "Requirement already satisfied: commentjson in /usr/local/lib/python3.7/dist-packages (from frimcla) (0.9.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from frimcla) (0.8.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from frimcla) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from frimcla) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from frimcla) (1.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.1.6->frimcla) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.1.6->frimcla) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras==2.1.6->frimcla) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.15.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.14.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (1.46.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->frimcla) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->frimcla) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->frimcla) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->frimcla) (3.3.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->frimcla) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->frimcla) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->frimcla) (4.2.0)\n",
            "Requirement already satisfied: lark-parser<0.8.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from commentjson->frimcla) (0.7.8)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->frimcla) (1.5.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->frimcla) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->frimcla) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->frimcla) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->frimcla) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->frimcla) (2022.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->frimcla) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->frimcla) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->frimcla) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->frimcla) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->frimcla) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->frimcla) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->frimcla) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install frimcla\n",
        "!pip install mahotas \n",
        "\n",
        "!pip install -U numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b0hlDz6CUFIF"
      },
      "outputs": [],
      "source": [
        "#os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dzI78Q0pMQN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fCDohp5OIUuG"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import mahotas\n",
        "import cv2\n",
        "import os\n",
        "from os import listdir\n",
        "import h5py\n",
        "\n",
        "\n",
        "#--------------------\n",
        "# tunable-parameters\n",
        "#--------------------\n",
        "images_per_class       = 17064\n",
        "fixed_size             = tuple((500, 500))\n",
        "train_path             = \"/content/drive/MyDrive/2022dataset 05 2 22/train\"\n",
        "h5_train_data          = '/content/drive/MyDrive/2022dataset 05 2 22/output/train_data.h5'\n",
        "h5_train_labels        = '/content/drive/MyDrive/2022dataset 05 2 22/output/train_labels.h5'\n",
        "bins                   = 8\n",
        "\n",
        "DEBUG = True #Show print statments\n",
        "\n",
        "# Converting each image to RGB from BGR format\n",
        "def rgb_bgr(image):\n",
        "    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return rgb_img\n",
        "\n",
        "# Conversion to HSV image format from RGB\n",
        "def bgr_hsv(rgb_img):\n",
        "    hsv_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
        "    return hsv_img\n",
        "\n",
        "# image segmentation\n",
        "# for extraction of green and brown color\n",
        "def img_segmentation(rgb_img,hsv_img):\n",
        "    lower_green = np.array([25,0,20])\n",
        "    upper_green = np.array([100,255,255])\n",
        "    healthy_mask = cv2.inRange(hsv_img, lower_green, upper_green)\n",
        "    result = cv2.bitwise_and(rgb_img,rgb_img, mask=healthy_mask)\n",
        "    lower_brown = np.array([10,0,10])\n",
        "    upper_brown = np.array([30,255,255])\n",
        "    disease_mask = cv2.inRange(hsv_img, lower_brown, upper_brown)\n",
        "    disease_result = cv2.bitwise_and(rgb_img, rgb_img, mask=disease_mask)\n",
        "    final_mask = healthy_mask + disease_mask\n",
        "    final_result = cv2.bitwise_and(rgb_img, rgb_img, mask=final_mask)\n",
        "    return final_result\n",
        "# feature-descriptor-1: Hu Moments\n",
        "def fd_hu_moments(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
        "    return feature\n",
        "# feature-descriptor-2: Haralick Texture\n",
        "def fd_haralick(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
        "    return haralick\n",
        "    \n",
        "# feature-descriptor-3: Color Histogram\n",
        "def fd_histogram(image, mask=None):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    return hist.flatten()\n",
        "# get the training labels\n",
        "train_labels = os.listdir(train_path)\n",
        "\n",
        "# sort the training labels\n",
        "train_labels.sort()\n",
        "print(train_labels)\n",
        "\n",
        "# empty lists to hold feature vectors and labels\n",
        "global_features = []\n",
        "labels          = []   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mPDUPchim8yn"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# loop over the training data sub-folders\n",
        "for training_name in train_labels:\n",
        "    # join the training data path and each species training folder\n",
        "    dir = os.path.join(train_path, training_name)\n",
        "\n",
        "    # get the current training label\n",
        "    current_label = training_name\n",
        "    counter =0\n",
        "\n",
        "    # loop over the images in each sub-folder\n",
        "    for x in range(1,images_per_class+1):\n",
        "        # get the image file name\n",
        "        file = dir + \"/\" + str(x) + \".jpg\"\n",
        "        if DEBUG:\n",
        "          print(\"INPUT FILES: \" + file)\n",
        "        # read the image and resize it to a fixed-size\n",
        "        image = cv2.imread(file)\n",
        "        try:\n",
        "          image = cv2.resize(image, fixed_size)\n",
        "        except cv2.error:\n",
        "          print(\"Error in file: \"+ str(file) )\n",
        "          if image:\n",
        "            print(str(image.shape))\n",
        "          continue\n",
        "        # Running Function Bit By Bit\n",
        "        RGB_BGR       = rgb_bgr(image)\n",
        "        BGR_HSV       = bgr_hsv(RGB_BGR)\n",
        "        IMG_SEGMENT   = img_segmentation(RGB_BGR, BGR_HSV)\n",
        "\n",
        "        # Call for Global Fetaure Descriptors\n",
        "        fv_hu_moments = fd_hu_moments(IMG_SEGMENT)\n",
        "        fv_haralick   = fd_haralick(IMG_SEGMENT)\n",
        "        fv_histogram  = fd_histogram(IMG_SEGMENT)\n",
        "        \n",
        "        #Show segmented images\n",
        "        if counter<5:\n",
        "          numpy_horizontal = np.hstack((image, IMG_SEGMENT))\n",
        "          cv2_imshow(numpy_horizontal)\n",
        "          counter+=1\n",
        "\n",
        "        # numpy_horizontal_concat = np.concatenate((image, grey_3_channel), axis=1)\n",
        "\n",
        "        # plt.figure(figsize=(10,10))\n",
        "        # counter = 0\n",
        "\n",
        "        # if counter<10:\n",
        "        #   plt.subplot(121)\n",
        "        #   plt.imshow(image)\n",
        "\n",
        "        #   plt.subplot(122)\n",
        "        #   plt.imshow(IMG_SEGMENT)\n",
        "        #   counter +=1\n",
        "\n",
        "        # Concatenate \n",
        "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
        "        # update the list of labels and feature vectors\n",
        "        labels.append(current_label)\n",
        "        global_features.append(global_feature)\n",
        "\n",
        "print(\"[STATUS] processed folder: {}\".format(current_label))\n",
        "print(\"[STATUS] completed Global Feature Extraction...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuYrDpWaptTO"
      },
      "outputs": [],
      "source": [
        "#!pip freeze\n",
        "\n",
        "# get the overall feature vector size\n",
        "print(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n",
        "\n",
        "# get the overall training label size\n",
        "print(\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n",
        "# encode the target labels\n",
        "targetNames = np.unique(labels)\n",
        "le          = LabelEncoder()\n",
        "target      = le.fit_transform(labels)\n",
        "if DEBUG:\n",
        "  print(\"ENCODED TARGET: \"+ str(target))\n",
        "print(\"[STATUS] training labels encoded...\")\n",
        "\n",
        "# scale features in the range (0-1)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled_features = scaler.fit_transform(global_features)\n",
        "print(\"[STATUS] feature vector normalized...\")\n",
        "print(\"[STATUS] target labels: {}\".format(target))\n",
        "print(\"[STATUS] target labels shape: {}\".format(target.shape))\n",
        "\n",
        "# save the feature vector using HDF5\n",
        "with h5py.File('myfile.hdf5','w') as f:\n",
        "    group = f.create_group('a_group')\n",
        "    group.create_dataset(name='matrix', data=np.zeros((10, 10)), chunks=True, compression='gzip')\n",
        "\n",
        "h5_train_data          = '/content/drive/MyDrive/2022dataset 05 2 22/output/train_data.h5'\n",
        "h5_train_labels        = '/content/drive/MyDrive/2022dataset 05 2 22/output/train_labels.h5'\n",
        "\n",
        "h5f_data  = h5py.File(h5_train_data,'w')\n",
        "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
        "\n",
        "h5f_label = h5py.File(h5_train_labels,'w')\n",
        "h5f_label.create_dataset('dataset_1', data=np.array(target))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ5p-cZT68Ey"
      },
      "outputs": [],
      "source": [
        "!pip install frimcla\n",
        "!pip install mahotas \n",
        "!pip install -U numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QklEhkHEyB7_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# training\n",
        "#-----------------------------------\n",
        "# TRAINING OUR MODEL\n",
        "#-----------------------------------\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "#from sklearn.externals import joblib\n",
        "import joblib\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#--------------------\n",
        "# tunable-parameters\n",
        "#--------------------\n",
        "num_trees = 100\n",
        "test_size = 0.20\n",
        "seed      = 9\n",
        "train_path = \"/content/drive/MyDrive/2022dataset 05 2 22/train\"\n",
        "test_path  = \"/content/drive/MyDrive/2022dataset 05 2 22/validation\"\n",
        "h5_train_data          = '/content/drive/MyDrive/2022dataset 05 2 22/output/train_data.h5'\n",
        "h5_train_labels        = '/content/drive/MyDrive/2022dataset 05 2 22/output/train_labels.h5'\n",
        "\n",
        "scoring    = \"accuracy\"\n",
        "\n",
        "# get the training labels\n",
        "train_labels = os.listdir(train_path)\n",
        "\n",
        "# sort the training labels\n",
        "train_labels.sort()\n",
        "\n",
        "if not os.path.exists(test_path):\n",
        "    os.makedirs(test_path)\n",
        "\n",
        "# create all the machine learning models\n",
        "models = []\n",
        "#models.append(('LR', LogisticRegression(random_state=seed)))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis() ))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier(random_state=seed)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(random_state=seed, probability=True))) #Added probability=True\n",
        "# models.append(('SVM', SVC(random_state=seed)))\n",
        "\n",
        "# variables to hold the results and names\n",
        "results = []\n",
        "names   = []\n",
        "\n",
        "# import the feature vector and trained labels\n",
        "h5f_data  = h5py.File(h5_train_data, 'r')\n",
        "h5f_label = h5py.File(h5_train_labels, 'r')\n",
        "\n",
        "global_features_string = h5f_data['dataset_1']\n",
        "global_labels_string   = h5f_label['dataset_1']\n",
        "\n",
        "global_features = np.array(global_features_string)\n",
        "global_labels   = np.array(global_labels_string)\n",
        "\n",
        "h5f_data.close()\n",
        "h5f_label.close()\n",
        "\n",
        "# verify the shape of the feature vector and labels\n",
        "print(\"[STATUS] features shape: {}\".format(global_features.shape))\n",
        "print(\"[STATUS] labels shape: {}\".format(global_labels.shape))\n",
        "\n",
        "print(\"[STATUS] training started...\")\n",
        "# split the training and testing data\n",
        "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n",
        "                                                                                          np.array(global_labels), test_size=test_size, random_state=seed)\n",
        "\n",
        "print(\"[STATUS] splitted train and test data...\")\n",
        "print(\"Train data  : {}\".format(trainDataGlobal.shape))\n",
        "print(\"Test data   : {}\".format(testDataGlobal.shape))\n",
        "\n",
        "trainDataGlobal\n",
        "for name, model in models:\n",
        "    kfold = KFold(n_splits=10, shuffle=True,random_state=seed)\n",
        "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison of machine learning algorithms')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()\n",
        "\n",
        "plt.savefig('Comparison of machine learning algorithms.pdf', dpi=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hwlycFgnkEk"
      },
      "outputs": [],
      "source": [
        "#Calculating ROC and AUC metrics\n",
        "#REFER TO: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "i=0\n",
        "\n",
        "for name, model in models:\n",
        "  pyplot.figure(figsize=(70,6))\n",
        "  #predict probabilities\n",
        "  model.fit(trainDataGlobal, trainLabelsGlobal)\n",
        "  model_probs = model.predict_proba(testDataGlobal)\n",
        "  #keep positive outcome\n",
        "  model_probs = model_probs[:,1]\n",
        "  #calculate auc score\n",
        "  model_auc = roc_auc_score(testLabelsGlobal,model_probs)\n",
        "  #score\n",
        "  print(name+\": ROC AUC=%.3f\" % model_auc)\n",
        "\n",
        "  #calculate roc curves\n",
        "  model_fpr, model_tpr, _ = roc_curve(testLabelsGlobal, model_probs)\n",
        "\n",
        "  #plot roc_curve for the model\n",
        "  pyplot.subplot(171+i)\n",
        "  pyplot.plot(model_fpr, model_tpr, linestyle='--', color='r', label=\"%s ROC curve (area=%0.2f)\" % (name, model_auc))\n",
        "  #pyplot.fill(model_fpr, model_tpr, color='b', label='AUC')#AUC\n",
        "  #axis labels\n",
        "  pyplot.grid(linestyle='--')\n",
        "  pyplot.xlabel('False Positive Rate')\n",
        "  pyplot.ylabel('True Positive Rate')\n",
        "  #legend\n",
        "  pyplot.legend(loc=\"lower right\")\n",
        "  #show plot\n",
        "  pyplot.show()\n",
        "  i+=1\n",
        " \n",
        "#Notice: u will get an error whith svm initialize the class constructor with arg \"probability=True\" to fix it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gHq2KV0TR3y"
      },
      "outputs": [],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5odTD1oPrKI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# boxplot algorithm comparison\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison of machine learning algorithms')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.savefig('Comparison of machine learning algorithms.pdf', dpi=1000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZJ-wAGta8zB"
      },
      "outputs": [],
      "source": [
        "#Coorected this for you to get figures with better quality (I remember the remark from last article)\n",
        "from google.colab import files\n",
        "fig.savefig(\"fig1.eps\", format=\"eps\") #u can still change eps to png but eps is recommended in articles\n",
        "files.download(\"fig1.eps\")\n",
        "# files.download('Comparison of machine learning algorithms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIq1cMwTEBp2"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "for name, model in models:\n",
        "  # model.fit(trainDataGlobal, trainLabelsGlobal)\n",
        "  y_predict=model.predict(testDataGlobal)\n",
        "  y_predict = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "  labels = ['Diseased', 'Healthy'] \n",
        "  cm = confusion_matrix(testLabelsGlobal,y_predict)\n",
        "  ax = fig.add_subplot(111)\n",
        "  confusion=sns.heatmap(cm ,annot=True, fmt ='g', ax=ax,  xticklabels=labels, yticklabels=labels,cmap=\"viridis\")\n",
        "  print(cm)\n",
        "  \n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  cax= ax.matshow(cm)\n",
        "  plt.title('Confusion matrix ' + name)\n",
        "  fig.colorbar(cax)\n",
        "  ax.set_xticklabels(['']+labels)\n",
        "  ax.set_yticklabels(['']+labels)\n",
        "  figure = confusion.get_figure()    \n",
        "  figure.savefig('confusion_ML.pdf', dpi=800)\n",
        "  #plt.xlabel('Predicted')\n",
        "  #plt.ylabel('True')\n",
        "  plt.show()\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxdMIdILr4YW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "for name, model in models:\n",
        "   y_predict=model.predict(testDataGlobal)\n",
        "   print(classification_report(testLabelsGlobal,y_predict ,target_names=['Diseased', 'default'], digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUSiai-xCC_f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(testLabelsGlobal, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYLxnRSdEA6I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}